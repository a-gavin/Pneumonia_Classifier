20200522_statusreport.txt

Matthew worked on the addition of a ResNet implementation that allows for convenient modifications to the blocks that support the architecture. This addition will allow us to create our own custom blocks with layers that we see fit and quickly compare them against our other baselines. Future work will be done to explore different block variations, the base performances of the blocks, and the performances after applying our data augmentation techniques.
Raleigh has finalized the baseline as well as added the proper performance metrics to the ResNet implementation. Going forward she will be working alongside Matthew to determine the final structure of each of the custom blocks. 
Alex implemented a script which in conjunction with logging software, wandb, will allow the group to perform a variety of data augmentation experiments, log the results, and analyze them all in one place. The scripting functionality will be essential in running large scale tests as the logging and analysis of data is greatly simplified by wandb. This will allow the team to analyze results as they are happening from a central dashboard and determine promising augmentations for subsequent experiments.
Chris and Ivan have come up with a set of data augmentations and have implemented helper code to augment the data for data loading and training purposes. Any combination of the following data augmentations are available to our models: optionally random 90-degree rotations, contrast boost, brightness boost, optionally random horizontal and vertical mirroring. Chris and Ivan have now moved on to optimizing the hyperparameters for the baseline, so we have a certainty that the baseline architecture is being used at its limits.
